Change log:

1.1.0

  - Added Jython 2.7 support.

  - Refactored agents to allow easier use of different prediction/learning algorithms.

    Prediction/learning algorithms that inherit from a new base Model class may now be used in an
    pluggable fashion with agents that inherit from a new MC_AIXI_Agent class.

    The MC_AIXI_Agent class uses the Monte Carlo search behaviour of the existing MC_AIXI_CTW_Agent class,
    but otherwise passes off prediction and learning to the model through Model class methods.

    Please see the rewritten MC_AIXI_CTW class for a reference in how to write new agents to take
    advantage of this new interface.

  - Added the ability to compare agent performance against other agents with the '-c/--compare' option.

    - Added a new agent, aixi_uniform_random, that performs uniformally-random actions.
      (This is useful in determining whether a prediction algorithm or agent performs better than random chance.)

    - In verbose logging mode, a variety of statistical tests will now be performed every 50 cycles.

    - The '-n/--non-learning-only' option allows these statistical tests to only be performed in the
      non-learning/non-exploration/post-training part of the training session, giving a clearer view of trained
      performance.

  - Added support for domains/environments with large (i.e. non-listable, large numeric valued) action,
    observation, and reward ranges.

    - Added new configuration options and defaults for new and previously-hardwired constants:

      - mc-action-search-limit:  how many (randomly chosen) actions to evaluate in a non-listable action range
                                 where exploration of each action isn't feasible.
                                 Default of 10.

      - mc-exploration-constant: how much weighting to give to re-exploration of already explored action nodes.
                                 Default of 2.0.

      - mc-unexplored-bias:      how much weighting to give to exploration of unexplored action nodes.
                                 Default of 1000000000.0.

  - the (original) agent to use can now be overridden on the command line using the new '-a/--agent' option.

  - Various small bug fixes to improve reliability, including ensuring the random seed is only set when that value is configured.

1.0.4:

  Added support for Python 3.x, and fixed an issue with how the random seed was set when no seed value is supplied.

1.0.3:

  Rearranged the package's directory hierarchy and module imports to fix import issues with
  the aixi.py script when run from the final installed location.

1.0.2:

  Improved messaging and documentation about long pauses during (non-random-exploration phase)
  action selection.

1.0.1:

  Fixed imports so that this package works out of the box on all platforms without installation.

1.0.0:

  Initial import of Python code based on the C++ MC-AIXI-CTW implementation at
  https://github.com/moridinamael/mc-aixi making the following changes in translation:

  - main.cpp/aixi executable to aixi.py:

    - The Python script equivalent takes optional command-line configuration options that override the
      option value with the same name from the given configuration file.

    - Added agent specification into the configuration options, for specifying the use of an
      agent based on a (new) base Agent class.

    - Any class can be used as an environment, so long as it inherits from the base Environment
      class, and if it's located within the Python system/package search path.

    - Added a profiling option ('-p'/'--profile') to generate execution-time statistics useful
      in finding which parts of the algorithm are consuming the most time, for subsequent
      optimisation work.

  - agent.cpp to agent.py and agents/mc_aixi_ctw.py:

    - split into two parts: a base Agent class in agent.py, and the parts more specific
      to the MC-AIXI-CTW algorithm in agents/mc_aixi_ctw.py.

    - the base Agent class is intended for agent classes to use to inherit (and override) basic
      environment interoperability methods.

    - accessor methods replaced with direct property access for simplicity.
      (If needed later, these properties can be transparently turned into calls to accessors.)

  - environment.cpp to environment.py:
  - search.cpp to search/monte_carlo_search_tree.py:

    - accessor methods replaced with direct property access for simplicity.

  - predict.cpp to prediction/ctw_context_tree.py:

    - tweaked the algorithm to cache the size of the context tree in the top-level tree object,
      as this provides a significant time performance improvement during exploration, with
      no observed performance decrease in other circumstances.

    - made smaller, non-algorithmic tweaks to improve time performance in this critical path
      code.

    - accessor methods replaced with direct property access for simplicity.

  - util.cpp to util.py:

    - translated only functions that didn't already have a Python equivalent.

    - added a enumeration generating function to replicate C++ enumeration types, while
      also providing membership checks and iteration over the range of defined values.